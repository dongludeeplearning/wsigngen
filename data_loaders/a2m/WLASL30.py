'''
This code is used to load the smplx parameters data generated by Hand4Whole.
Author : Lu Dong
Created : 2023-05-30 
Modified: 2023-10-08
Copyright (c) 2023 by Lu Dong <ludong@buffalo.edu>, All Rights Reserved.
'''
import pickle as pkl
import numpy as np
import pdb
import json
import os
from tqdm import tqdm
from .dataset import Dataset
from pdb import set_trace as st
import glob
import re 



# datapath = '/home/lu/Research3/Data_Collection/'
# datapath = '/home/lu/Research3/Data_Collection/Ablation/frames'   # abalation
# datapath = '/home/lu/Research3/Data_Collection/ASLGloss100/gloss_video_split_frames_smplx100'

class WLASL30(Dataset):
    dataname = "wlasl30"
    datapath = '/home/lu/Research/Research3/Data_Collection/'

    def __init__(self, split, datapath=datapath, **kargs):
        self.datapath = datapath
        self._jointsIx = 0
        # self.num_actions = 30 
        super().__init__(**kargs)
        
        samples_pose = []  # colllect all video poses
               
        labels = []

        #set_flag
        load_pkl= True # false means read from scratch; true means load pkl file
        save_raw= False  # true means save for next time; false means load pkl 
        save_raw_label= False
        

        if not load_pkl:
            print(f'Load dataset from seperate npz files')
            label_path= './WLASL_v0.3.json'  
            with open(label_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)  

                # loop the data folder train split==train
                for video_id in tqdm(os.listdir(os.path.join(datapath, split)), desc = "Loading " + split + " dataset"): 
                    videopath= os.path.join(datapath, split, video_id)     
                    print(f'Staring process {split} video {video_id}')
                    
                    # glob video npz seq 
                    npz_file_format = r'\d{4}_smplx\.npz'
                    sub_files = glob.glob(os.path.join(videopath,'*'))
                    matching_npz = [file for file in sub_files if re.match(npz_file_format, os.path.basename(file))]
                    npz_seq_paths = sorted (matching_npz)
                                                             
                    # loop through video folder
                    for index, frame_npz in enumerate(npz_seq_paths) :
                        # load npz 
                        frame_dict= np.load(frame_npz, allow_pickle=True)                       
                        # parse the npz files
                        root_pose= frame_dict['root_pose'] #(1,3)
                        body_pose=frame_dict['body_pose'].reshape(21, 3)   # 21
                        lhand_pose=frame_dict['lhand_pose'].reshape(15, 3)  #15
                        rhand_pose=frame_dict['rhand_pose'].reshape(15, 3) #15
                        jaw_pose=frame_dict['jaw_pose'] # (1.3)
                        # shape_para = frame_dict['shape'] #(1,10) not_use 
                        # expr_para= frame_dict['expr'] #(1,10) not_use 
                        
                        full_pose=np.vstack((root_pose,body_pose,lhand_pose,rhand_pose,jaw_pose))   #(53,3)
                        full_pose = np.expand_dims(full_pose, axis=0)   #(1,53,3)  numpy
                 
                        if index == 0:                       
                            output = full_pose                           
                        else:    
                            output = np.vstack([output, full_pose])  # collect all_frames for each video_id
                            
                    samples_pose.append(output)  # collect all the videos under train folder
                    label = get_json_label(json_data, video_id)
                    labels.append(label)

            if save_raw:
                # save the load data for 1st use
                save_pkl='/home/lu/Research/Research3/Data_Collection/ASLGloss30_Dataset'

                save_poses = os.path.join(save_pkl, split, 'samples_pose.pkl')
                with open(save_poses, "wb") as fs:
                    pkl.dump(samples_pose,fs)
                    print(f"save all {split } sampls pose succ!")

                save_label= os.path.join(save_pkl, split, 'samples_label.pkl')
                with open(save_label, "wb") as fl:
                    pkl.dump(labels,fl)
                    print(f"save all {split} samples label succ!")

        else:
            print("Data already exist, loading pkl file from "+ split)
            # save_pkl='/home/lu/Research3/Data_Collection/ASLGloss30_Dataset'
            
            save_pkl= os.path.join(datapath, 'ASLGloss30_Dataset')
            load_poses = os.path.join(save_pkl, split, 'samples_pose.pkl')
            load_labels =  os.path.join(save_pkl, split, 'samples_label.pkl')
            with open(load_poses, "rb") as fs:
                samples_pose=pkl.load(fs)
            with open(load_labels , "rb") as fl:
                labels=pkl.load(fl)

                                                  
        # get video_level pose sequence
        self._pose = [x for x in samples_pose]  # get pose seq under one video  [(84, 53, 3, 3)..]
        self._num_frames_in_video = [len(p) for p in self._pose]  #[84, 46, 39, 50, 58, 31, 113, 38, 30, 45, 59, 72, 41, 45, 23, 49, 74, 81, 32, 63, 159]
        self._joints = [ ] # get joint3d seq under one video  # [(84, 144, 3), ]
        self._actions = [x for x in labels]  # all the labels ['abdomen', 'africa', 'afraid', 'against', 'age', 'age']
        self._unique_actions = list(set([x for x in labels]))  # remove the repeateed one  ['africa', 'able', 'after', 'abdomen', 'afternoon', 'against']

        # self.num_classes = len(self._unique_actions )
        self.num_actions = len(self._unique_actions )
      
        self._train = list(range(len(self._pose)))  # get train video length as idx
        self._test = list(range(len(self._pose))) 
        
        action_classes_file = os.path.join(datapath, 'ASLGloss30_Dataset/Gloss_introduction/gloss_label_30.txt' )
        if os.path.exists(action_classes_file ):
            # If the file exists, read its contents into self._action_classes
            with open(action_classes_file, 'r') as f:
                self._action_classes = np.array(f.read().splitlines())
            print (f"Read action classes from {action_classes_file}")
        else: 
            # If the file doesn't exist, create it and write the word list
            with open(action_classes_file, 'w') as file:
                for gloss in self._unique_actions:
                    file.write( gloss + '\n')
            self._action_classes = np.array(self._unique_actions)
            print(f"File '{action_classes_file}' didn't exist and has been created with the unique actions.")

        a2l_path =   os.path.join(datapath,'ASLGloss30_Dataset/Gloss_introduction/action2label_h4w_30.txt')
        l2a_path =   os.path.join(datapath,'ASLGloss30_Dataset/Gloss_introduction/label2action_h4w_30.txt') 
        use_hum= True
        if use_hum:
            a2d_path = '/home/lu/Research/Research3/SignDiffusion/data_collection/HumEva/action2description_30_mason.txt'  # add long text  
        else:
            a2d_path =  os.path.join(datapath,'ASLGloss30_Dataset/Gloss_introduction/action2description_30.txt') # add long text  

        if save_raw_label:  
            # for 1st time use 
            self._action_to_label = {x: i for i, x in enumerate(self._unique_actions)}   #{'africa': 0, 'able': 1, 'after': 2, 'abdomen': 3, 'afternoon': 4, 'against': 5, 'affect': 6, 'a': 7, 'afraid': 8, 'age': 9, 'again': 10}
            self._label_to_action = {i: x for i, x in enumerate(self._unique_actions)}   #{0: 'africa', 1: 'able', 2: 'after', 3: 'abdomen', 4: 'afternoon', 5: 'against', 6: 'affect', 7: 'a', 8: 'afraid', 9: 'age', 10: 'again'}

            with open(a2l_path, "w")as f1:
                json.dump(self._action_to_label, f1)
            with open(l2a_path, "w")as f2:
                json.dump(self._label_to_action, f2)
            with open(l2a_path, "w")as f3:
                json.dump(self._action2description, f3)

        else: 
            # directly load from here
            with open (a2l_path, "r") as f1 :
                self._action_to_label= json.load(f1)
            with open (l2a_path, "r") as f2 :
                self._label_to_action= json.load(f2) 
            with open (a2d_path , "r") as f3 :
                self._action2description = json.load(f3) 
                

    def _load_rotvec(self, ind, frame_ix):   
        # use video_index and frame_index to get frame-pose parameters
        
        pose = self._pose[ind][frame_ix].reshape(-1, 53, 3)
        return pose




def get_json_label(data, video_id):
    # use the video_id to look up label 
    for idx in data:
        instances = idx['instances']
        label = idx['gloss']
        for instance in instances:
            if video_id.split(".")[0] == instance['video_id']:
                return label



if __name__ == "__main__":

    dataset = WLASL30(split="train")

    